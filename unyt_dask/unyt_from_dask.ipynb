{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `yt`, `unyt` and `dask` ! \n",
    "\n",
    "In this notebook, we present an initial attempt at creating a `dask` Custom Collection for `unyt` arrays. As of `yt4.0`, the `yt.units` functionality [was extracted into the `unyt` package](https://yt-project.org/docs/dev/yt4differences.html?highlight=ytarray#yt-units-is-now-a-wrapper-for-unyt). The base [`unyt` arrays](https://unyt.readthedocs.io/en/stable/modules/unyt.array.html) are used throughout `yt` for tracking and converting units and so in order to fully leverage `dask` within `yt` it would be helpful to have `unyt` arrays with `dask` functionality. \n",
    "\n",
    "In general, `dask` builds task graphs to operate on `collections`. Some commonly used `collections` are `dask.array`, `dask.dataframe` and `dask.bag`. Each of these `collections` is subclassed off of the general `DaskMethodsMixin` collection and the dask documentation provides an overview of how to add [custom collections](https://docs.dask.org/en/latest/custom-collections.html#example-dask-collection). So what we want is a new collection, `unyt_dask_array`, that can be used for buliding and execulting dask task graphs in parallel while preseving `unyt_array` functionality. \n",
    "\n",
    "For the present problem, both `unyt` and `dask.array` wrap numpy methods and so the question is how do we cleverly wrap and sublcass to leverage the existing wrapped methods? The `dask` numpy wrappings account for the necessary distribution and reduction operations between chunks and so the simpler place to start is by subclassing `dask.array` and adding `unyt` functionality alongside. Tracking units does not have to happen on each chunk and so in the following, we track units and a cumulative unit conversion factor within our new `unyt_dask_array` class separately and only apply the final conversion when returning the result of a `dask.compute`. \n",
    "\n",
    "So let's just dump some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unyt.array import unyt_array\n",
    "\n",
    "from dask.array.core import Array, finalize\n",
    "import numpy as np\n",
    "\n",
    "def unyt_from_dask(dask_array,\n",
    "              units = None,\n",
    "              registry = None,\n",
    "              dtype = None,\n",
    "              bypass_validation = False,\n",
    "              input_units = None,\n",
    "              name = None):\n",
    "    (cls, args) = dask_array.__reduce__()\n",
    "    da = unyt_dask_array(*args)\n",
    "    da._attach_units(units, registry, dtype, bypass_validation, input_units, name)\n",
    "    return da\n",
    "\n",
    "def finalize_unyt(results,unit_name,factor):\n",
    "    # the function to call for the __dask_postcompute__ hook. \n",
    "    return unyt_array(finalize(results)*factor,unit_name)\n",
    "\n",
    "class unyt_dask_array(Array):\n",
    "    def __init__(self, dask_graph, name, chunks, dtype=None, meta=None, shape=None):\n",
    "        self.units = None\n",
    "        self.unyt_name = None\n",
    "        self.dask_name = name\n",
    "        self.factor = 1.\n",
    "\n",
    "    def _attach_units(self,units = None,\n",
    "              registry = None,\n",
    "              dtype = None,\n",
    "              bypass_validation = False,\n",
    "              input_units = None,\n",
    "              name = None):\n",
    "        x_np = np.array([1.])\n",
    "        self._unyt_array = unyt_array(x_np, units, registry, dtype, bypass_validation, input_units, name)\n",
    "        self.units = self._unyt_array.units\n",
    "        self.unyt_name = self._unyt_array.name\n",
    "\n",
    "    def to(self, units, equivalence=None, **kwargs):\n",
    "        # tracks any time units are converted with a running conversion factor\n",
    "        # that gets applied after calling dask methods\n",
    "        init_val = self._unyt_array.value[0]\n",
    "        self._unyt_array = self._unyt_array.to(units, equivalence, **kwargs)\n",
    "        self.factor = self.factor * self._unyt_array.value[0] / init_val\n",
    "        self.units = units\n",
    "        self.unyt_name = self._unyt_array.name\n",
    "\n",
    "    def min(self, axis=None, keepdims=False, split_every=None, out=None):\n",
    "        result = np.array(super().min(axis, keepdims, split_every, out))\n",
    "        return unyt_array(result*self.factor, self.units)\n",
    "\n",
    "    def max(self, axis=None, keepdims=False, split_every=None, out=None):\n",
    "        result = np.array(super().max(axis, keepdims, split_every, out))\n",
    "        return unyt_array(result*self.factor, self.units)\n",
    "\n",
    "    def __dask_postcompute__(self):\n",
    "        # a dask hook to catch after .compute(), see\n",
    "        # https://docs.dask.org/en/latest/custom-collections.html#example-dask-collection\n",
    "        # but it does not catch all computes?\n",
    "        return finalize_unyt, ((self.units, self.factor))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we define three objects: the `unyt_from_dask` function, the `finalize_unyt` function  and the `unyt_dask_array` class. Let's focus on the new class first. \n",
    "\n",
    "In our new subclass, `unyt_dask_array(Array)`, `Array` is the core array class of `dask`. This class only has a `__new__` constructor, and so in the `__init__` here we only provide the arguments that get sent to `__new__`: \n",
    "\n",
    "\n",
    "```python\n",
    "def __init__(self, dask_graph, name, chunks, dtype=None, meta=None, shape=None):\n",
    "        self.units = None\n",
    "        self.unyt_name = None\n",
    "        self.dask_name = name\n",
    "        self.factor = 1.\n",
    "```\n",
    "\n",
    "all those arguments are those needed for the base `Array.__new__` constructor, and when we instantiate `unyt_dask_array` the super-class's `__new__` will be called with those arguments automatically before proceeding with `unyt_dask_array.__init__()`. \n",
    "\n",
    "These arguments are all related to the details of how dask constructs its graphs and chunks, but we want to be able to instantiate our `unyt_dask_array` more simply. Thus, the convenience function `unyt_from_dask` constructs our new `Array` subclass from an existing `dask` array without having to know the details of how `dask` works.\n",
    "\n",
    "So, for example, we can do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 800.00 MB </td> <td> 8.00 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (10000, 10000) </td> <td> (1000, 1000) </td></tr>\n",
       "    <tr><th> Count </th><td> 100 Tasks </td><td> 100 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"120\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"120\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"120\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"120\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"120\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"120\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"120\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"120\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"120\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"120\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"120\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"120\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"120\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"120\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">10000</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import unyt; import dask.array as da\n",
    "\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x_da = unyt_from_dask(x, unyt.m)\n",
    "x_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which behaves as a `dask` array. e.g.,: \n",
    "\n",
    "For example, if we load everything into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array([[0.84873849, 0.49570262, 0.84942177, ..., 0.76667334,\n",
       "             0.88406054, 0.06651547],\n",
       "            [0.23614276, 0.2691459 , 0.7563452 , ..., 0.82625261,\n",
       "             0.79109846, 0.09371664],\n",
       "            [0.85376455, 0.64940279, 0.13865977, ..., 0.06566836,\n",
       "             0.46294204, 0.79582689],\n",
       "            ...,\n",
       "            [0.67687777, 0.02797173, 0.58873372, ..., 0.58259256,\n",
       "             0.69760201, 0.88294463],\n",
       "            [0.47230504, 0.9908476 , 0.09871617, ..., 0.58093535,\n",
       "             0.31079139, 0.81819524],\n",
       "            [0.30721092, 0.38728382, 0.51681618, ..., 0.55783229,\n",
       "             0.66753928, 0.17387946]], 'm')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get a `unyt_array`! This happens because of the `unyt_dask_array.__dask_postcompute__()` hook tells dask to call the `finalize_unyt` function after we execute `compute`. This function simply calls the `finalize` function used by the core dask `Array` and initializes a `unyt_array` with the resulting numpy array:\n",
    "\n",
    "```\n",
    "def finalize_unyt(results,unit_name,factor):\n",
    "    # the function to call for the __dask_postcompute__ hook. \n",
    "    return unyt_array(finalize(results)*factor,unit_name)\n",
    "```\n",
    "\n",
    "The `factor` is a cumulative unit conversion factor, [described below](#tracking-units).\n",
    "\n",
    "One caveat to the `__dask_postcompute__` hook is that it does not seem to always catch. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 8 B </td> <td> 8 B </td></tr>\n",
       "    <tr><th> Shape </th><td> () </td> <td> () </td></tr>\n",
       "    <tr><th> Count </th><td> 239 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<sum-aggregate, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002711.294516616"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not catch and we just get a scalar factor. We do still have the units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but ideally we'd be returning a `unyt_array` or `unyt_quantity` here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to catch these operations, we can subclass appropriately... For example, the `unyt_dask_array` class has the following methods that call the corresponding superclass methods for `min` and `max` and then convert to a standard `ndarray` and then return a  `unyt_array`:\n",
    "\n",
    "```python\n",
    "    def min(self, axis=None, keepdims=False, split_every=None, out=None):\n",
    "        result = np.array(super().min(axis, keepdims, split_every, out))\n",
    "        return unyt_array(result*self.factor, self.units)\n",
    "\n",
    "    def max(self, axis=None, keepdims=False, split_every=None, out=None):\n",
    "        result = np.array(super().max(axis, keepdims, split_every, out))\n",
    "        return unyt_array(result*self.factor, self.units)\n",
    "```    \n",
    "\n",
    "So when we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array(1.56775319e-08, 'm')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get our standard `unyt_array`. \n",
    "\n",
    "\n",
    "### tracking units \n",
    "\n",
    "We're also using a bit of trickery to deal with unit conversions in this class within the `unyt_dask_array.to` method, copied here:\n",
    "\n",
    "```python\n",
    "def to(self, units, equivalence=None, **kwargs):\n",
    "        # tracks any time units are converted with a running conversion factor\n",
    "        # that gets applied after calling dask methods\n",
    "        init_val = self._unyt_array.value[0]\n",
    "        self._unyt_array = self._unyt_array.to(units, equivalence, **kwargs)\n",
    "        self.factor = self.factor * self._unyt_array.value[0] / init_val\n",
    "        self.units = units\n",
    "        self.unyt_name = self._unyt_array.name\n",
    "```        \n",
    "\n",
    "so within our `unyt_dask_array`, we initialize a hidden `unyt_array` with a value of single value of `1`. Then any time a conversion occours, we apply the conversion to this `self._unyt_array` and track a cumulative conversion factor. Now, whenever we return calculations from `dask` into memory, we simply multiply by this conversion factor and attach the appropriate units. There is likely a more elegant solution here, but the basic idea is to track the units separately from the dask functionaly as the dask operations on each chunk are generally independent of the units scaling. \n",
    "\n",
    "So here's an example conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_da.to(unyt.km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "km"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array(1.56775319e-11, 'km')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_da.to(unyt.nanometer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array(15.67753194, 'nm')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which we see our units changing appropriately. This approach is nice because when we convert multiple times: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_da.to(unyt.cm)\n",
    "x_da.to(unyt.micrometer)\n",
    "x_da.to(unyt.km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are only operating on the hidden `_unyt_array` to track what the final conversion factor should be. In this case, we're converting back to our original units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array(1.56775319e-11, 'km')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty with this method is that we want to avoid manually subclassing the many methods in the base `Array` collection -- there is likely a clever way to automatically wrap all the methods. Furthermore, we are not using `unyt`'s architecture much at all here, and things may become more complicated, for example, when multiplying two `dask_unyt_array` objects together. But this seems a good start!\n",
    "\n",
    "### parallel!\n",
    "\n",
    "Now, because our `unyt_from_dask` class is built off of a `dask` collection, it will work with the parallel scheduling. \n",
    "\n",
    "So let's spin up a client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(threads_per_worker=2, n_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33059</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>33.51 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:33059' processes=2 threads=4, memory=33.51 GB>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and re-instantiate our arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 800.00 MB </td> <td> 8.00 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (10000, 10000) </td> <td> (1000, 1000) </td></tr>\n",
       "    <tr><th> Count </th><td> 100 Tasks </td><td> 100 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"120\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"120\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"120\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"120\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"120\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"120\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"120\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"120\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"120\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"120\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"120\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"120\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"120\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"120\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">10000</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x_da = unyt_from_dask(x, unyt.m)\n",
    "x_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now when we compute our properties, `dask` will compute values from chunks independently. So when we take a min:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array(1.47933353e-08, 'm')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on our `dask` dashboard, we can see the distributed tasks complete via the Task Graph:\n",
    "\n",
    "\n",
    "![TaskStream](resources/unyt_dask_taskgraph.png)\n",
    "\n",
    "\n",
    "In the above example for `min` and `max`, we are converting the result from the superclass call to a standard `ndarray` as the results here will generally be small enough to be held in memory, even when returning an array using the `axis` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array([0.99986894, 0.99992408, 0.99952803, ..., 0.99995384,\n",
       "            0.99989531, 0.99997846], 'm')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, when we convert units, we don't actually touch the dask array chunks but track a cumulative conversion factor using a hidden `_unyt_array` with a single value. So when we convert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_da.to(unyt.cm)\n",
    "x_da.to(unyt.micrometer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nothing gets distributed to the dask chunks. The conversion factor only gets applied to the chunks **after** the reduction step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_array([999868.94269632, 999924.07710443, 999528.02896929, ...,\n",
       "            999953.84159969, 999895.31102362, 999978.45862321], 'μm')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_da.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in general, this method shows a fairly straightforward approach for adding dask support to `unyt` which can be in turn leveraged by `yt`. The main work to be done is to devise a more clever way to wrap the `dask.array` methods to attach the final units in order to avoid manually subclassing each method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
