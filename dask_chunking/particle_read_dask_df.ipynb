{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a more streamlined dask data loader of particle data that could be implemented directly in `BaseIOHandler._read_particle_selection()`. The summary is that we can create dask data frames from delayed reads of chunks! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt \n",
    "from dask import dataframe as ddf, delayed\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yt : [INFO     ] 2020-10-20 16:20:20,642 Files located at /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,643 Default to loading snap_033.0.hdf5 for snapshot_033 dataset\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,699 Parameters: current_time              = 4.343952725460923e+17 s\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,700 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,700 Parameters: domain_left_edge          = [0. 0. 0.]\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,700 Parameters: domain_right_edge         = [25. 25. 25.]\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,701 Parameters: cosmological_simulation   = 1\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,701 Parameters: current_redshift          = -4.811891664902035e-05\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,701 Parameters: omega_lambda              = 0.762\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,701 Parameters: omega_matter              = 0.238\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,702 Parameters: omega_radiation           = 0.0\n",
      "yt : [INFO     ] 2020-10-20 16:20:20,702 Parameters: hubble_constant           = 0.73\n"
     ]
    }
   ],
   "source": [
    "ds = yt.load_sample(\"snapshot_033\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a quick mock chunk assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yt : [INFO     ] 2020-10-20 16:20:22,079 Allocating for 4.194e+06 particles\n",
      "Loading particle index: 100%|██████████| 12/12 [00:00<00:00, 213.98it/s]\n"
     ]
    }
   ],
   "source": [
    "class MockChunkObject:\n",
    "    def __init__(self, data_file):\n",
    "        self.data_files = [data_file]\n",
    "        \n",
    "class MockChunk:\n",
    "    def __init__(self, data_file):\n",
    "        self.objs = [MockChunkObject(data_file)]\n",
    "        \n",
    "chunks = [MockChunk(data_file) for data_file in ds.index.data_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now let's set choose some particle types and fields: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptf = {'PartType0':['Density','Nitrogen'],\n",
    "       'PartType4':['Oxygen','MetallicityWeightedRedshift']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = ds.sphere('max',5.) \n",
    "selector = sp.selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we're going to use `dask.dataframe.from_delayed` to create a dask dataframe. Given that fields of different particle types within a single chunk may not have the same length, we will read in by particle type. Additionally, since some chunks may not have particles at all, we need to pass `from_delayed` a `meta` schema declaring the types of the field, otherwise aggregation operations across chunks may fail. So let's create a `meta` dictionary for each particle type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the meta dictionary for each chunk's dataframe so that empty chunks \n",
    "# don't cause problems. \n",
    "ptype_meta = {}\n",
    "for ptype,flds in ptf.items():\n",
    "    meta_dict = {}\n",
    "    for fld in flds: \n",
    "        meta_dict[fld] = pd.Series([],dtype=np.float64)\n",
    "    ptype_meta[ptype] = meta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a function to delay -- this function needs to return a dataframe for a single chunk and particle type with columns for each field: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_single_ptype(chunk, this_ptf, selector, meta_dict):\n",
    "        # read into a pandas dataframe so that we can use dask.dataframe.from_delayed!        \n",
    "        chunk_results = pd.DataFrame(meta_dict)        \n",
    "        for field_r, vals in ds.index.io._read_particle_fields([chunk], this_ptf, selector):\n",
    "            chunk_results[field_r[1]] = vals\n",
    "\n",
    "        return chunk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create our delayed dataframes! We'll store each particle type in a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptypes = list(ptf.keys()) \n",
    "delayed_dfs = {}\n",
    "for ptype in ptypes: \n",
    "    # build a dataframe from delayed for each particle type\n",
    "    this_ptf = {ptype:ptf[ptype]}\n",
    "    delayed_chunks = [\n",
    "        delayed(_read_single_ptype)(ch, this_ptf, selector, ptype_meta[ptype])\n",
    "        for ch in chunks\n",
    "    ]\n",
    "    delayed_dfs[ptype] = ddf.from_delayed(delayed_chunks,meta=ptype_meta[ptype])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we've got a dict of delayed dask dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PartType0': Dask DataFrame Structure:\n",
       "                 Density Nitrogen\n",
       " npartitions=12                  \n",
       "                 float64  float64\n",
       "                     ...      ...\n",
       " ...                 ...      ...\n",
       "                     ...      ...\n",
       "                     ...      ...\n",
       " Dask Name: from-delayed, 24 tasks,\n",
       " 'PartType4': Dask DataFrame Structure:\n",
       "                  Oxygen MetallicityWeightedRedshift\n",
       " npartitions=12                                     \n",
       "                 float64                     float64\n",
       "                     ...                         ...\n",
       " ...                 ...                         ...\n",
       "                     ...                         ...\n",
       "                     ...                         ...\n",
       " Dask Name: from-delayed, 24 tasks}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delayed_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can pull out a single delayed frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oxygen</th>\n",
       "      <th>MetallicityWeightedRedshift</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=12</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 24 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 Oxygen MetallicityWeightedRedshift\n",
       "npartitions=12                                     \n",
       "                float64                     float64\n",
       "                    ...                         ...\n",
       "...                 ...                         ...\n",
       "                    ...                         ...\n",
       "                    ...                         ...\n",
       "Dask Name: from-delayed, 24 tasks"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = delayed_dfs['PartType4']\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "until this point, everything is still delayed. but when we perform operations like `mean()` and `sum()`, dask will handle all the cross-chunk aggregation. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=float64>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.MetallicityWeightedRedshift.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is still delayed. So let's compute it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.963667205141687"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.MetallicityWeightedRedshift.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01397729, 1.55316567, 2.71978283, ..., 0.        , 2.81968474,\n",
       "       1.96937144])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.MetallicityWeightedRedshift.values.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Won't work in parallel tho, as dask will try to pickle up `ds.index.io._read_particle_fields`. The selector object can be pickled (with the changes on `pickleableSelects`), but `ds.index.io._read_particle_fields` likely needs some pickle fixes... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BaseIOHandler._read_particle_selection` expects a dictionary of arrays on output, which we can read into memory with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = {}\n",
    "for ptype in ptypes:\n",
    "    for col in delayed_dfs[ptype].columns:\n",
    "        df = delayed_dfs[ptype]\n",
    "        rv[(ptype, col)] = df[col].values.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('PartType0',\n",
       "  'Density'): array([1.18035096e+08, 7.38237760e+08, 6.64672064e+08, ...,\n",
       "        8.06066418e+00, 7.77778673e+00, 7.48051739e+00]),\n",
       " ('PartType0',\n",
       "  'Nitrogen'): array([0.00184869, 0.00545696, 0.00762129, ..., 0.        , 0.00028541,\n",
       "        0.        ]),\n",
       " ('PartType4',\n",
       "  'Oxygen'): array([0.0438178 , 0.01489225, 0.01456669, ..., 0.        , 0.00435656,\n",
       "        0.00267334]),\n",
       " ('PartType4',\n",
       "  'MetallicityWeightedRedshift'): array([1.01397729, 1.55316567, 2.71978283, ..., 0.        , 2.81968474,\n",
       "        1.96937144])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
