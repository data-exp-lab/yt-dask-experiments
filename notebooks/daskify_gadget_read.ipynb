{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's copy over the `read_gadget.py` contents and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yt : [WARNING  ] 2020-09-11 14:37:27,760 tqdm is not installed, progress bar can not be displayed.\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,270 Files located at /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,271 Default to loading snap_033.0.hdf5 for snapshot_033 dataset\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,326 Parameters: current_time              = 4.343952725460923e+17 s\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,327 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,327 Parameters: domain_left_edge          = [0. 0. 0.]\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,328 Parameters: domain_right_edge         = [25. 25. 25.]\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,328 Parameters: cosmological_simulation   = 1\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,328 Parameters: current_redshift          = -4.811891664902035e-05\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,328 Parameters: omega_lambda              = 0.762\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,329 Parameters: omega_matter              = 0.238\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,329 Parameters: omega_radiation           = 0.0\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,330 Parameters: hubble_constant           = 0.73\n",
      "yt : [INFO     ] 2020-09-11 14:37:28,384 Allocating for 4.194e+06 particles\n",
      "Loading particle index: 100%|██████████| 12/12 [00:00<00:00, 193.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import yt\n",
    "\n",
    "ds = yt.load_sample(\"snapshot_033\")\n",
    "\n",
    "reg = ds.all_data()\n",
    "\n",
    "class MockSelector:\n",
    "    is_all_data = True\n",
    "\n",
    "class MockChunkObject:\n",
    "    def __init__(self, data_file):\n",
    "        self.data_files = [data_file]\n",
    "\n",
    "class MockChunk:\n",
    "    def __init__(self, data_file):\n",
    "        self.objs = [MockChunkObject(data_file)]\n",
    "\n",
    "ptf = {'PartType0': ['Coordinates']}\n",
    "\n",
    "chunks = [MockChunk(data_file) for data_file in ds.index.data_files]\n",
    "selector= MockSelector()\n",
    "\n",
    "my_gen = ds.index.io._read_particle_fields(chunks, ptf, selector)\n",
    "my_result = [_ for _ in my_gen]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what comes out of this is a list with an entry for each chunk: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('PartType0', 'Coordinates'),\n",
       " array([[ 7.6320577 , 11.81454   ,  0.5112596 ],\n",
       "        [ 7.630863  , 11.814384  ,  0.51114064],\n",
       "        [ 7.633304  , 11.81966   ,  0.51152855],\n",
       "        ...,\n",
       "        [ 9.0948305 , 18.531418  , 13.523693  ],\n",
       "        [ 9.084332  , 18.547832  , 13.502992  ],\n",
       "        [ 9.102446  , 18.544275  , 13.524328  ]], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `ds.index.data_files` objects is a list of `ParticleFile` objects with info about how that chunk including the on-disc datafile and start/end indices within that datafile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5\n",
      "[0, 262144]\n",
      "/home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5\n",
      "[262144, 280105]\n",
      "/home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.1.hdf5\n",
      "[0, 262144]\n"
     ]
    }
   ],
   "source": [
    "for fid in [0,1,2]:\n",
    "    print(ds.index.data_files[fid].filename)\n",
    "    print([ds.index.data_files[fid].start,ds.index.data_files[fid].end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several ideas for applying dask:\n",
    "\n",
    "* parrallelize: some dask.delayed() decorating to parallelize without relying on MPI\n",
    "* daskify the read_particle_fields function to return dask arrays? \n",
    "\n",
    "Let's try writing a `read_particle_fields` that we can mess with by copying the `gadget` one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yt.frontends.gadget.io.IOHandlerGadgetHDF5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds.index.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def read_particle_fields(self,chunks, ptf, selector):\n",
    "        # Now we have all the sizes, and we can allocate\n",
    "        data_files = set([])\n",
    "        for chunk in chunks:\n",
    "            for obj in chunk.objs:\n",
    "                data_files.update(obj.data_files)\n",
    "                fls=','.join([x.filename for x in obj.data_files])\n",
    "                print(f'adding {fls}')\n",
    "                \n",
    "        for data_file in sorted(data_files, key=lambda x: (x.filename, x.start)):\n",
    "            si, ei = data_file.start, data_file.end\n",
    "            f = h5py.File(data_file.filename, mode=\"r\")\n",
    "            for ptype, field_list in sorted(ptf.items()):\n",
    "                if data_file.total_particles[ptype] == 0:\n",
    "                    continue\n",
    "                g = f[f\"/{ptype}\"]\n",
    "                if getattr(selector, \"is_all_data\", False):\n",
    "                    mask = slice(None, None, None)\n",
    "                    mask_sum = data_file.total_particles[ptype]\n",
    "                    hsmls = None\n",
    "                else:\n",
    "                    coords = g[\"Coordinates\"][si:ei].astype(\"float64\")\n",
    "                    if ptype == \"PartType0\":\n",
    "                        hsmls = self._get_smoothing_length(\n",
    "                            data_file, g[\"Coordinates\"].dtype, g[\"Coordinates\"].shape\n",
    "                        ).astype(\"float64\")\n",
    "                    else:\n",
    "                        hsmls = 0.0\n",
    "                    mask = selector.select_points(\n",
    "                        coords[:, 0], coords[:, 1], coords[:, 2], hsmls\n",
    "                    )\n",
    "                    if mask is not None:\n",
    "                        mask_sum = mask.sum()\n",
    "                    del coords\n",
    "                if mask is None:\n",
    "                    continue\n",
    "                for field in field_list:\n",
    "                    if field in (\"Mass\", \"Masses\") and ptype not in self.var_mass:\n",
    "                        data = np.empty(mask_sum, dtype=\"float64\")\n",
    "                        ind = self._known_ptypes.index(ptype)\n",
    "                        data[:] = self.ds[\"Massarr\"][ind]\n",
    "                    elif field in self._element_names:\n",
    "                        rfield = \"ElementAbundance/\" + field\n",
    "                        data = g[rfield][si:ei][mask, ...]\n",
    "                    elif field.startswith(\"Metallicity_\"):\n",
    "                        col = int(field.rsplit(\"_\", 1)[-1])\n",
    "                        data = g[\"Metallicity\"][si:ei, col][mask]\n",
    "                    elif field.startswith(\"GFM_Metals_\"):\n",
    "                        col = int(field.rsplit(\"_\", 1)[-1])\n",
    "                        data = g[\"GFM_Metals\"][si:ei, col][mask]\n",
    "                    elif field.startswith(\"Chemistry_\"):\n",
    "                        col = int(field.rsplit(\"_\", 1)[-1])\n",
    "                        data = g[\"ChemistryAbundances\"][si:ei, col][mask]\n",
    "                    elif field == \"smoothing_length\":\n",
    "                        # This is for frontends which do not store\n",
    "                        # the smoothing length on-disk, so we do not\n",
    "                        # attempt to read them, but instead assume\n",
    "                        # that they are calculated in _get_smoothing_length.\n",
    "                        if hsmls is None:\n",
    "                            hsmls = self._get_smoothing_length(\n",
    "                                data_file,\n",
    "                                g[\"Coordinates\"].dtype,\n",
    "                                g[\"Coordinates\"].shape,\n",
    "                            ).astype(\"float64\")\n",
    "                        data = hsmls[mask]\n",
    "                    else:\n",
    "                        data = g[field][si:ei][mask, ...]\n",
    "\n",
    "                    yield (ptype, field), data\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now call the new one, passing in the `ds.index.io` object as `self`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.1.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.1.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.2.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.2.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.3.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.3.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.4.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.5.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.6.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.7.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('PartType0', 'Coordinates'),\n",
       "  array([[ 7.6320577 , 11.81454   ,  0.5112596 ],\n",
       "         [ 7.630863  , 11.814384  ,  0.51114064],\n",
       "         [ 7.633304  , 11.81966   ,  0.51152855],\n",
       "         ...,\n",
       "         [ 9.0948305 , 18.531418  , 13.523693  ],\n",
       "         [ 9.084332  , 18.547832  , 13.502992  ],\n",
       "         [ 9.102446  , 18.544275  , 13.524328  ]], dtype=float32)),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  array([[ 9.0948925, 18.540127 , 13.505761 ],\n",
       "         [ 9.099401 , 18.551332 , 13.511906 ],\n",
       "         [ 9.082766 , 18.54066  , 13.496419 ],\n",
       "         ...,\n",
       "         [ 9.948605 ,  8.47677  , 14.566635 ],\n",
       "         [ 9.948661 ,  8.478258 , 14.567051 ],\n",
       "         [ 9.94791  ,  8.478077 , 14.566901 ]], dtype=float32)),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  array([[ 1.5066416 ,  0.0244492 ,  3.2759523 ],\n",
       "         [ 1.5392694 ,  0.05701507,  3.292754  ],\n",
       "         [ 1.5390531 ,  0.05318429,  3.30687   ],\n",
       "         ...,\n",
       "         [11.684051  , 10.220002  , 19.18336   ],\n",
       "         [11.671257  , 10.222958  , 19.174152  ],\n",
       "         [11.676265  , 10.222806  , 19.190056  ]], dtype=float32))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_gen = ds.index.io._read_particle_fields(chunks, ptf, selector)\n",
    "my_gen = read_particle_fields(ds.index.io, chunks, ptf, selector)\n",
    "my_result = [_ for _ in my_gen]\n",
    "my_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "huh, that actually worked. cool. let's explore some ways to use dask here.... start with loading the data into dask as soon as it's read from disk. let's try to return the data for each chunk as sub-chunked dask arrays. To do that we'll need to read the data from disk into a dask array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.index.data_files[0].filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does dask's read_hdf work here?\n",
    "```\n",
    "dask.dataframe.read_hdf(pattern, key, start=0, stop=None, columns=None, chunksize=1000000, sorted_index=False, lock=True, mode='a')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as df, array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turns out it won't work: likely a pandas hdf issue -- seems there's a common issue (e.g. [this](https://github.com/dask/dask/issues/747)) in which pandas can only read very specific hdf files.... so this errors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gkey = \"/PartType0\"\n",
    "df.read_hdf(ds.index.data_files[0].filename,gkey,mode='r')\n",
    "```\n",
    "\n",
    "`TypeError: cannot create a storer if the object is not existing nor a value are passed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "but we can try reading with h5py and loading as a dask array following https://docs.dask.org/en/latest/array-creation.html#numpy-slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(ds.index.data_files[0].filename, mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkey = \"/PartType0\"\n",
    "coords = f[gkey]['Coordinates'][ds.index.data_files[0].start:ds.index.data_files[0].end].astype(\"float64\")\n",
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 6.29 MB </td> <td> 240.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (262144, 3) </td> <td> (30000, 1) </td></tr>\n",
       "    <tr><th> Count </th><td> 28 Tasks </td><td> 27 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"13\" x2=\"25\" y2=\"13\" />\n",
       "  <line x1=\"0\" y1=\"27\" x2=\"25\" y2=\"27\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"54\" x2=\"25\" y2=\"54\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"25\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"109\" x2=\"25\" y2=\"109\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"8\" y1=\"0\" x2=\"8\" y2=\"120\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"16\" y2=\"120\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">262144</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(262144, 3), dtype=float64, chunksize=(30000, 1), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_da = da.from_array(coords, chunks=(30000, 1))\n",
    "coords_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that would be a sub-chunk of a single base chunk (the base chunk == the file with the start/end indeces, here we're splitting that up). The above dask docs link claims \"This process is entirely lazy. Neither creating the h5py object nor wrapping it with da.from_array have loaded any data.\" hmm... but `coords` here should be in memory? confused... in any case:\n",
    "\n",
    "Let's assemble a delayed dask list of sub-chunked chunks? \n",
    "\n",
    "First let's write a new read function to return dask arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_particle_fields_dask(self,chunks, ptf, selector):\n",
    "    \n",
    "        # let's still loop over the chunks \n",
    "        data_files = set([])\n",
    "        for chunk in chunks:\n",
    "            for obj in chunk.objs:\n",
    "                data_files.update(obj.data_files)\n",
    "                fls=','.join([x.filename for x in obj.data_files])\n",
    "                print(f'adding {fls}')\n",
    "                \n",
    "        # and we still loop over each base chunk  \n",
    "        for data_file in sorted(data_files, key=lambda x: (x.filename, x.start)):\n",
    "            si, ei = data_file.start, data_file.end\n",
    "            f = h5py.File(data_file.filename, mode=\"r\")\n",
    "            \n",
    "            for ptype, field_list in sorted(ptf.items()):\n",
    "                if data_file.total_particles[ptype] == 0:\n",
    "                    continue\n",
    "                g = f[f\"/{ptype}\"]\n",
    "                if getattr(selector, \"is_all_data\", False):\n",
    "                    mask = slice(None, None, None)\n",
    "                    mask_sum = data_file.total_particles[ptype]\n",
    "                    hsmls = None\n",
    "                else:\n",
    "                    coords = g[\"Coordinates\"][si:ei].astype(\"float64\")\n",
    "                    if ptype == \"PartType0\":\n",
    "                        hsmls = self._get_smoothing_length(\n",
    "                            data_file, g[\"Coordinates\"].dtype, g[\"Coordinates\"].shape\n",
    "                        ).astype(\"float64\")\n",
    "                    else:\n",
    "                        hsmls = 0.0\n",
    "                    mask = selector.select_points(\n",
    "                        coords[:, 0], coords[:, 1], coords[:, 2], hsmls\n",
    "                    )\n",
    "                    if mask is not None:\n",
    "                        mask_sum = mask.sum()\n",
    "                    del coords\n",
    "                if mask is None:\n",
    "                    continue\n",
    "                for field in field_list:\n",
    "                    if field in (\"Mass\", \"Masses\") and ptype not in self.var_mass:\n",
    "                        data = np.empty(mask_sum, dtype=\"float64\")\n",
    "                        ind = self._known_ptypes.index(ptype)\n",
    "                        data[:] = self.ds[\"Massarr\"][ind]\n",
    "                    elif field in self._element_names:\n",
    "                        rfield = \"ElementAbundance/\" + field\n",
    "                        data = g[rfield][si:ei][mask, ...]\n",
    "                    elif field.startswith(\"Metallicity_\"):\n",
    "                        col = int(field.rsplit(\"_\", 1)[-1])\n",
    "                        data = g[\"Metallicity\"][si:ei, col][mask]\n",
    "                    elif field.startswith(\"GFM_Metals_\"):\n",
    "                        col = int(field.rsplit(\"_\", 1)[-1])\n",
    "                        data = g[\"GFM_Metals\"][si:ei, col][mask]\n",
    "                    elif field.startswith(\"Chemistry_\"):\n",
    "                        col = int(field.rsplit(\"_\", 1)[-1])\n",
    "                        data = g[\"ChemistryAbundances\"][si:ei, col][mask]\n",
    "                    elif field == \"smoothing_length\":\n",
    "                        # This is for frontends which do not store\n",
    "                        # the smoothing length on-disk, so we do not\n",
    "                        # attempt to read them, but instead assume\n",
    "                        # that they are calculated in _get_smoothing_length.\n",
    "                        if hsmls is None:\n",
    "                            hsmls = self._get_smoothing_length(\n",
    "                                data_file,\n",
    "                                g[\"Coordinates\"].dtype,\n",
    "                                g[\"Coordinates\"].shape,\n",
    "                            ).astype(\"float64\")\n",
    "                        data = hsmls[mask]\n",
    "                    else:\n",
    "                        data = g[field][si:ei][mask, ...]\n",
    "\n",
    "                  \n",
    "                    if data.ndim > 1:\n",
    "                        subchunk_shape = (10000,1)  # dont chunk up multidim arrays like Coordinates\n",
    "                    else:\n",
    "                        subchunk_shape = (10000)  \n",
    "                        \n",
    "                    yield (ptype, field), da.from_array(data,chunks=subchunk_shape)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.0.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.1.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.1.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.2.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.2.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.3.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.3.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.4.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.5.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.6.hdf5\n",
      "adding /home/chavlin/hdd/data/yt_data/yt_sample_sets/snapshot_033.tar.gz.untar/snapshot_033/snap_033.7.hdf5\n"
     ]
    }
   ],
   "source": [
    "my_gen = read_particle_fields_dask(ds.index.io, chunks, ptf, selector)\n",
    "my_result = [_ for _ in my_gen]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now each of those data chunks is a dask array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array.core.Array"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_result[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 3.15 MB </td> <td> 40.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (262144, 3) </td> <td> (10000, 1) </td></tr>\n",
       "    <tr><th> Count </th><td> 82 Tasks </td><td> 81 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> >f4 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"4\" x2=\"25\" y2=\"4\" />\n",
       "  <line x1=\"0\" y1=\"9\" x2=\"25\" y2=\"9\" />\n",
       "  <line x1=\"0\" y1=\"13\" x2=\"25\" y2=\"13\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"25\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"27\" x2=\"25\" y2=\"27\" />\n",
       "  <line x1=\"0\" y1=\"32\" x2=\"25\" y2=\"32\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"25\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"41\" x2=\"25\" y2=\"41\" />\n",
       "  <line x1=\"0\" y1=\"45\" x2=\"25\" y2=\"45\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"54\" x2=\"25\" y2=\"54\" />\n",
       "  <line x1=\"0\" y1=\"59\" x2=\"25\" y2=\"59\" />\n",
       "  <line x1=\"0\" y1=\"64\" x2=\"25\" y2=\"64\" />\n",
       "  <line x1=\"0\" y1=\"68\" x2=\"25\" y2=\"68\" />\n",
       "  <line x1=\"0\" y1=\"73\" x2=\"25\" y2=\"73\" />\n",
       "  <line x1=\"0\" y1=\"77\" x2=\"25\" y2=\"77\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"86\" x2=\"25\" y2=\"86\" />\n",
       "  <line x1=\"0\" y1=\"91\" x2=\"25\" y2=\"91\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"25\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"105\" x2=\"25\" y2=\"105\" />\n",
       "  <line x1=\"0\" y1=\"109\" x2=\"25\" y2=\"109\" />\n",
       "  <line x1=\"0\" y1=\"114\" x2=\"25\" y2=\"114\" />\n",
       "  <line x1=\"0\" y1=\"119\" x2=\"25\" y2=\"119\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"8\" y1=\"0\" x2=\"8\" y2=\"120\" />\n",
       "  <line x1=\"16\" y1=\"0\" x2=\"16\" y2=\"120\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 25.412617,0.000000 25.412617,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">262144</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(262144, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(262144, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(419, 3), dtype=>f4, chunksize=(419, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(255819, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(251598, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(244445, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(239908, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(233206, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(227868, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>),\n",
       " (('PartType0', 'Coordinates'),\n",
       "  dask.array<array, shape=(225819, 3), dtype=>f4, chunksize=(10000, 1), chunktype=numpy.ndarray>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 120 B </td> <td> 40 B </td></tr>\n",
       "    <tr><th> Shape </th><td> (10, 3) </td> <td> (10, 1) </td></tr>\n",
       "    <tr><th> Count </th><td> 85 Tasks </td><td> 3 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> >f4 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"93\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"43\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"43\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"14\" y1=\"0\" x2=\"14\" y2=\"120\" />\n",
       "  <line x1=\"29\" y1=\"0\" x2=\"29\" y2=\"120\" />\n",
       "  <line x1=\"43\" y1=\"0\" x2=\"43\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 43.544815,0.000000 43.544815,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"21.772407\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >3</text>\n",
       "  <text x=\"63.544815\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,63.544815,60.000000)\">10</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<getitem, shape=(10, 3), dtype=>f4, chunksize=(10, 1), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[0][1][:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.6320577 , 11.81454   ,  0.5112596 ],\n",
       "       [ 7.630863  , 11.814384  ,  0.51114064],\n",
       "       [ 7.633304  , 11.81966   ,  0.51152855],\n",
       "       [ 7.6330523 , 11.819399  ,  0.51174545],\n",
       "       [ 7.6328063 , 11.818984  ,  0.512373  ],\n",
       "       [ 7.634123  , 11.820499  ,  0.5114557 ],\n",
       "       [ 7.6352324 , 11.820305  ,  0.5110059 ],\n",
       "       [ 7.634844  , 11.817485  ,  0.5118982 ],\n",
       "       [ 7.6348486 , 11.817831  ,  0.51274544],\n",
       "       [ 7.635442  , 11.818255  ,  0.5099228 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[0][1][:10,:].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so then let's try using dask to compute a derived quantity in parallel? Let's find a mean? One chunk would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.328807"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[0][1].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786432"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result[0][1].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so not in parallel, we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.217880717413463\n"
     ]
    }
   ],
   "source": [
    "meanval=0.\n",
    "count=0. \n",
    "for result in my_result:\n",
    "    count+=result[1].size\n",
    "    meanval+=result[1].sum().compute()\n",
    "meanval=meanval / count \n",
    "print(meanval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to parallelize, we can assemble some dask delayed tasks and compute them after initializing a dask.distributed Client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Delayed('Array-430f3401-c06f-46f9-9596-ab27fd2b8f7b'),\n",
       " Delayed('Array-45bb94a2-7b38-426a-850f-c6db0829e661'),\n",
       " Delayed('Array-38a586ec-c6ea-4308-a435-b63a52405f89'),\n",
       " Delayed('Array-1dd5bf3f-4893-42ef-aa01-438e0dfcb426'),\n",
       " Delayed('Array-52dc0457-72df-4755-812f-fcba24cd5b9a'),\n",
       " Delayed('Array-be983546-b61f-4639-870c-10cf4ab9a870'),\n",
       " Delayed('Array-a825a648-a095-4141-adc7-5ed279422219'),\n",
       " Delayed('Array-1d6c0abc-2a89-4bc0-afad-85fbad18bb0e'),\n",
       " Delayed('Array-7e4c26f4-f4ae-41e0-8b5d-8ca87b802d03')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask import delayed,compute\n",
    "\n",
    "\n",
    "sums = [] \n",
    "count=0. \n",
    "for result in my_result:\n",
    "    count+=result[1].size\n",
    "    sums.append(delayed(result[1].sum()))\n",
    "    \n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client \n",
    "c = Client(threads_per_worker=1,n_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "summed = np.sum(compute(*sums))/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.21788017812798"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findmin(chunks):\n",
    "    \n",
    "    # assemble the delayed operation \n",
    "    minvals = []\n",
    "    for result in my_result:\n",
    "        minvals.append(delayed(result[1].min()))\n",
    "\n",
    "    # return the min of each \n",
    "    return np.min(compute(*minvals))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findmin(my_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a trival example, but it seems to work. the minval of each chunk is calculated on separate processors and we compute the min over all the chunks after using `compute` to collect. \n",
    "\n",
    "## An aside about units \n",
    "\n",
    "Let's check what happens to a yt array when stored as a dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 80.00 kB </td> <td> 8.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (10000,) </td> <td> (1000,) </td></tr>\n",
       "    <tr><th> Count </th><td> 11 Tasks </td><td> 10 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> unyt.unyt_array </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"25\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"25\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"25\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,25.412617 0.000000,25.412617\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(10000,), dtype=float64, chunksize=(1000,), chunktype=unyt.unyt_array>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dasked_yt = da.from_array(yt.YTArray(np.random.random(10000),'kpc'),chunks=(1000,))\n",
    "dasked_yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5005705955532631"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dasked_yt.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so things work, but we lose the `unyt` attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unyt_quantity(0.49740046, 'kpc')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.YTArray(np.random.random(10000),'kpc').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
